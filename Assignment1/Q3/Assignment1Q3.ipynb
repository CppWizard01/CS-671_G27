{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxOygZmEkIyr",
        "outputId": "5265122c-74a0-4e01-e82e-32f48ce62255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "IMG_SIZE =64\n",
        "NUM_CLASSES=10\n",
        "\n",
        "NUM_TRAIN_SAMPLES=None\n",
        "NUM_VAL_SAMPLES =None\n",
        "\n",
        "BATCH_SIZE =128\n",
        "EPOCHS =10\n",
        "LR = 1e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXZ6T0A1mch4",
        "outputId": "661bf33d-7398-4c75-b167-515e1272c423"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87scuGtNYXvv"
      },
      "outputs": [],
      "source": [
        "source_drive_path = '/content/drive/MyDrive/tiny-imagenet.zip'\n",
        "local_zip_file_path = './tiny-imagenet-dataset/tiny-imagenet.zip'\n",
        "unzip_destination_path = './tiny-imagenet-dataset/tiny-imagenet'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de9e23c5"
      },
      "outputs": [],
      "source": [
        "os.makedirs(os.path.dirname(local_zip_file_path),exist_ok=True)\n",
        "shutil.copyfile(source_drive_path,local_zip_file_path)\n",
        "\n",
        "os.makedirs(unzip_destination_path,exist_ok=True)\n",
        "\n",
        "#Unzipping\n",
        "with zipfile.ZipFile(local_zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bdf76af9"
      },
      "outputs": [],
      "source": [
        "DATA_ROOT = './tiny-imagenet'\n",
        "TRAIN_DIR = os.path.join(DATA_ROOT, 'train')\n",
        "VAL_DIR = os.path.join(DATA_ROOT, 'val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5981f1fa",
        "outputId": "86ee4710-7579-49d3-a747-682364711955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training batches:28\n",
            "Number of validation batches:4\n"
          ]
        }
      ],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    # Normalising to get ==> Zero mean and unit variance\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform=transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset =datasets.ImageFolder(root=TRAIN_DIR, transform=train_transform)\n",
        "val_dataset =datasets.ImageFolder(root=VAL_DIR, transform=val_transform)\n",
        "\n",
        "if NUM_TRAIN_SAMPLES is not None and NUM_TRAIN_SAMPLES<len(train_dataset):\n",
        "    indices = np.random.choice(len(train_dataset), NUM_TRAIN_SAMPLES, replace=False)\n",
        "    train_dataset = torch.utils.data.Subset(train_dataset, indices)\n",
        "\n",
        "if NUM_VAL_SAMPLES is not None and NUM_VAL_SAMPLES < len(val_dataset):\n",
        "    indices = np.random.choice(len(val_dataset), NUM_VAL_SAMPLES, replace=False)\n",
        "    val_dataset = torch.utils.data.Subset(val_dataset, indices)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "print(f\"Number of training batches:{len(train_loader)}\")\n",
        "print(f\"Number of validation batches:{len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHkq2ebZknAf"
      },
      "source": [
        "# **Vanishing Gradients & Modern Fixes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6772680f"
      },
      "outputs": [],
      "source": [
        "class DeepFCN_Sigmoid(nn.Module):\n",
        "    def __init__(self, input_dim=3*IMG_SIZE*IMG_SIZE, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.fc1 =nn.Linear(input_dim, 4096)\n",
        "        self.fc2 =nn.Linear(4096, 4096)\n",
        "        self.fc3 =nn.Linear(4096, 2048)\n",
        "        self.fc4 =nn.Linear(2048, 1024)\n",
        "        self.fc5 =nn.Linear(1024, 512)\n",
        "        self.fc6 =nn.Linear(512, 256)\n",
        "        self.fc7 =nn.Linear(256, 128)\n",
        "        self.fc8 =nn.Linear(128, 128)\n",
        "        self.fc9 = nn.Linear(128, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Using Xavier initialisation\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    # Setting sigmoid activation fuction\n",
        "    def forward(self, x):\n",
        "        x =x.view(x.size(0), -1)\n",
        "        x =torch.sigmoid(self.fc1(x))\n",
        "        x =torch.sigmoid(self.fc2(x))\n",
        "        x =torch.sigmoid(self.fc3(x))\n",
        "        x =torch.sigmoid(self.fc4(x))\n",
        "        x =torch.sigmoid(self.fc5(x))\n",
        "        x =torch.sigmoid(self.fc6(x))\n",
        "        x =torch.sigmoid(self.fc7(x))\n",
        "        x=torch.sigmoid(self.fc8(x))\n",
        "        x =self.fc9(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c50N27Lmbcuw"
      },
      "outputs": [],
      "source": [
        "class DeepFCN_ReLU_BN(nn.Module):\n",
        "    def __init__(self, input_dim=3*IMG_SIZE*IMG_SIZE, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.input_dim =input_dim\n",
        "\n",
        "        self.fc1 =nn.Linear(input_dim, 4096)\n",
        "        self.bn1 =nn.BatchNorm1d(4096)\n",
        "        self.fc2 =nn.Linear(4096, 4096)\n",
        "        self.bn2 =nn.BatchNorm1d(4096)\n",
        "        self.fc3 = nn.Linear(4096, 2048)\n",
        "        self.bn3 =nn.BatchNorm1d(2048)\n",
        "        self.fc4 = nn.Linear(2048, 1024)\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\n",
        "        self.fc5 =nn.Linear(1024, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.fc6 =nn.Linear(512, 256)\n",
        "        self.bn6 =nn.BatchNorm1d(256)\n",
        "        self.fc7 = nn.Linear(256, 128)\n",
        "        self.bn7 =nn.BatchNorm1d(128)\n",
        "        self.fc8= nn.Linear(128, 128)\n",
        "        self.bn8 =nn.BatchNorm1d(128)\n",
        "        self.fc9 =nn.Linear(128, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Using Kaiming initialisation in case of relu actication function.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    #Using relu activation function\n",
        "    def forward(self, x):\n",
        "        x =x.view(x.size(0), -1)\n",
        "        x =F.relu(self.bn1(self.fc1(x)))\n",
        "        x =F.relu(self.bn2(self.fc2(x)))\n",
        "        x =F.relu(self.bn3(self.fc3(x)))\n",
        "        x =F.relu(self.bn4(self.fc4(x)))\n",
        "        x =F.relu(self.bn5(self.fc5(x)))\n",
        "        x =F.relu(self.bn6(self.fc6(x)))\n",
        "        x =F.relu(self.bn7(self.fc7(x)))\n",
        "        x=F.relu(self.bn8(self.fc8(x)))\n",
        "        x =self.fc9(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c4f06dce"
      },
      "outputs": [],
      "source": [
        "def train_one_experiment(model_cls, experiment_name, epochs=EPOCHS):\n",
        "    model = model_cls().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_acc\": [],\n",
        "        \"first_layer_grad_norm\": [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss=0.0\n",
        "        grad_norms = []\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images =images.to(device, non_blocking=True)\n",
        "            labels =labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            outputs =model(images)\n",
        "            loss =criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient norm of the first layer weights\n",
        "            first_layer=model.fc1\n",
        "            if first_layer.weight.grad is not None:\n",
        "                grad_norm =first_layer.weight.grad.detach().norm().item()\n",
        "                grad_norms.append(grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "            running_loss+=loss.item()*images.size(0)\n",
        "\n",
        "        epoch_loss=running_loss/len(train_loader.dataset)\n",
        "        avg_grad_norm =float(np.mean(grad_norms)) if grad_norms else 0.0\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images =images.to(device, non_blocking=True)\n",
        "                labels =labels.to(device, non_blocking=True)\n",
        "                outputs =model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_running_loss +=loss.item()*images.size(0)\n",
        "                _, preds = outputs.max(1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_loss =val_running_loss/len(val_loader.dataset)\n",
        "        val_acc = correct / total if total > 0 else 0.0\n",
        "\n",
        "        history[\"train_loss\"].append(epoch_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"first_layer_grad_norm\"].append(avg_grad_norm)\n",
        "\n",
        "        print(f\"[{experiment_name}] Epoch {epoch+1}/{epochs}-\"\n",
        "              f\"Train Loss: {epoch_loss:.4f},Val Loss:{val_loss:.4f},\"\n",
        "              f\"Val Acc: {val_acc*100:.2f}%, Grad Norm (fc1): {avg_grad_norm:.4e}\")\n",
        "\n",
        "    return history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I6pfxNida60",
        "outputId": "9b87807d-e2a1-4dc3-b0b4-39e8862120f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Sigmoid] Epoch 1/10-Train Loss: 2.3313,Val Loss:2.3056,Val Acc: 10.00%, Grad Norm (fc1): 4.1558e-05\n",
            "[Sigmoid] Epoch 2/10-Train Loss: 2.2894,Val Loss:2.2173,Val Acc: 18.00%, Grad Norm (fc1): 4.5082e-03\n",
            "[Sigmoid] Epoch 3/10-Train Loss: 2.0742,Val Loss:2.1643,Val Acc: 16.00%, Grad Norm (fc1): 9.8835e-02\n",
            "[Sigmoid] Epoch 4/10-Train Loss: 2.0269,Val Loss:1.9572,Val Acc: 18.20%, Grad Norm (fc1): 1.0742e-01\n",
            "[Sigmoid] Epoch 5/10-Train Loss: 1.9312,Val Loss:1.9174,Val Acc: 17.80%, Grad Norm (fc1): 7.6747e-02\n",
            "[Sigmoid] Epoch 6/10-Train Loss: 1.8690,Val Loss:1.9226,Val Acc: 21.40%, Grad Norm (fc1): 5.5991e-02\n",
            "[Sigmoid] Epoch 7/10-Train Loss: 1.8342,Val Loss:1.8815,Val Acc: 22.00%, Grad Norm (fc1): 5.3741e-02\n",
            "[Sigmoid] Epoch 8/10-Train Loss: 1.7986,Val Loss:1.8209,Val Acc: 22.00%, Grad Norm (fc1): 5.2134e-02\n",
            "[Sigmoid] Epoch 9/10-Train Loss: 1.7611,Val Loss:1.8196,Val Acc: 26.00%, Grad Norm (fc1): 4.7513e-02\n",
            "[Sigmoid] Epoch 10/10-Train Loss: 1.7230,Val Loss:1.8157,Val Acc: 27.20%, Grad Norm (fc1): 5.3965e-02\n"
          ]
        }
      ],
      "source": [
        "history_sigmoid = train_one_experiment(DeepFCN_Sigmoid, \"Sigmoid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8jnMoBOddth",
        "outputId": "e451b927-6f23-4346-9a7f-fe687f8b030b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ReLU+BN] Epoch 1/10-Train Loss: 1.9779,Val Loss:2.0080,Val Acc: 30.80%, Grad Norm (fc1): 1.9713e+00\n",
            "[ReLU+BN] Epoch 2/10-Train Loss: 1.6316,Val Loss:1.6574,Val Acc: 35.40%, Grad Norm (fc1): 1.0708e-01\n",
            "[ReLU+BN] Epoch 3/10-Train Loss: 1.5044,Val Loss:1.6129,Val Acc: 40.00%, Grad Norm (fc1): 1.2710e-01\n",
            "[ReLU+BN] Epoch 4/10-Train Loss: 1.4072,Val Loss:1.6096,Val Acc: 42.80%, Grad Norm (fc1): 1.6268e-01\n",
            "[ReLU+BN] Epoch 5/10-Train Loss: 1.3059,Val Loss:1.6343,Val Acc: 38.60%, Grad Norm (fc1): 1.8290e-01\n",
            "[ReLU+BN] Epoch 6/10-Train Loss: 1.2060,Val Loss:1.6854,Val Acc: 41.60%, Grad Norm (fc1): 2.4258e-01\n",
            "[ReLU+BN] Epoch 7/10-Train Loss: 1.1382,Val Loss:1.6188,Val Acc: 43.40%, Grad Norm (fc1): 2.5370e-01\n",
            "[ReLU+BN] Epoch 8/10-Train Loss: 0.9961,Val Loss:1.7442,Val Acc: 41.40%, Grad Norm (fc1): 3.0466e-01\n",
            "[ReLU+BN] Epoch 9/10-Train Loss: 0.8708,Val Loss:1.7720,Val Acc: 44.60%, Grad Norm (fc1): 3.3853e-01\n",
            "[ReLU+BN] Epoch 10/10-Train Loss: 0.8013,Val Loss:1.9348,Val Acc: 41.20%, Grad Norm (fc1): 3.7569e-01\n"
          ]
        }
      ],
      "source": [
        "history_relu_bn = train_one_experiment(DeepFCN_ReLU_BN, \"ReLU+BN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "b8a848b2",
        "outputId": "ec0c8e65-72fb-4541-940d-d8ca8d7f9c21"
      },
      "outputs": [],
      "source": [
        "# Plotting the gradient norms\n",
        "\n",
        "epochs_range = range(1, EPOCHS + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs_range,history_sigmoid[\"first_layer_grad_norm\"], marker=\"o\", label=\"Sigmoid\")\n",
        "plt.plot(epochs_range, history_relu_bn[\"first_layer_grad_norm\"], marker=\"s\", label=\"ReLU + BatchNorm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Gradient Norm (fc1 weights)\")\n",
        "plt.title(\"First-Layer Gradient Norms\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plotting validation accuracy to see training speed\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs_range, history_sigmoid[\"val_acc\"], marker=\"o\", label=\"Sigmoid\")\n",
        "plt.plot(epochs_range, history_relu_bn[\"val_acc\"], marker=\"s\", label=\"ReLU + BatchNorm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYEHeStFkIyw"
      },
      "source": [
        "### Interpretation: Which version trains faster and why?\n",
        "\n",
        "Based on the gradient-norm plot and the validation accuracy curves:\n",
        "\n",
        "- **Sigmoid network (Experiment A)**: The first-layer gradient norms typically become **very small** as training progresses, especially in such a deep network. Sigmoid activations squish inputs into \\((0, 1)\\), and their derivatives are at most 0.25 and often close to 0 in the saturated regions. Multiplying many of these small derivatives across 8+ layers leads to **vanishing gradients**, so early layers (like `fc1`) update extremely slowly, and training is slow/unstable.\n",
        "- **ReLU + BatchNorm network (Experiment B)**: ReLU has derivative 1 in its active region and 0 otherwise, so gradients do **not shrink multiplicatively** in the same way as sigmoids. Batch Normalization further keeps the intermediate activations well-scaled (zero mean, unit variance), which stabilizes gradients and allows for **larger, more consistent gradient norms in the first layer**.\n",
        "\n",
        "Therefore, **the ReLU + BatchNorm network trains faster**, both because:\n",
        "1. Gradients do *not* vanish across layers as severely (ReLU), and\n",
        "2. Internal covariate shift is reduced and activations stay in a good range (BatchNorm), enabling efficient backpropagation even in very deep fully-connected networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO5fIwpNkIyw"
      },
      "source": [
        "# **ABLATION STUDY**\n",
        "\n",
        "Our baseline will be ReLU + Batch Normalisation + Optimizer -- > Adam + LR= 1e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32ad25ab"
      },
      "outputs": [],
      "source": [
        "class DeepFCN_ReLU_BN_Dropout(nn.Module):\n",
        "    #Our baseline NN as we declared above.\n",
        "    def __init__(self, input_dim=3*IMG_SIZE*IMG_SIZE, num_classes=NUM_CLASSES, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.fc1 =nn.Linear(input_dim, 4096)\n",
        "        self.bn1 =nn.BatchNorm1d(4096)\n",
        "        self.dropout1 =nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.bn2 = nn.BatchNorm1d(4096)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.fc3 =nn.Linear(4096, 2048)\n",
        "        self.bn3 = nn.BatchNorm1d(2048)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "        self.fc4 =nn.Linear(2048, 1024)\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\n",
        "        self.dropout4 =nn.Dropout(dropout_rate)\n",
        "        self.fc5 =nn.Linear(1024, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.dropout5 = nn.Dropout(dropout_rate)\n",
        "        self.fc6 = nn.Linear(512, 256)\n",
        "        self.bn6 =nn.BatchNorm1d(256)\n",
        "        self.dropout6 = nn.Dropout(dropout_rate)\n",
        "        self.fc7 = nn.Linear(256, 128)\n",
        "        self.bn7= nn.BatchNorm1d(128)\n",
        "        self.dropout7 = nn.Dropout(dropout_rate)\n",
        "        self.fc8 =nn.Linear(128, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =x.view(x.size(0), -1)\n",
        "        x = self.dropout1(F.relu(self.bn1(self.fc1(x))))\n",
        "        x =self.dropout2(F.relu(self.bn2(self.fc2(x))))\n",
        "        x= self.dropout3(F.relu(self.bn3(self.fc3(x))))\n",
        "        x = self.dropout4(F.relu(self.bn4(self.fc4(x))))\n",
        "        x =self.dropout5(F.relu(self.bn5(self.fc5(x))))\n",
        "        x = self.dropout6(F.relu(self.bn6(self.fc6(x))))\n",
        "        x= self.dropout7(F.relu(self.bn7(self.fc7(x))))\n",
        "        x =self.fc8(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dDwhB8ufXyX"
      },
      "outputs": [],
      "source": [
        "class DeepFCN_ReLU_BN_NoDropout(nn.Module):\n",
        "    # Removing Dropout from our baseline model\n",
        "    def __init__(self, input_dim=3*IMG_SIZE*IMG_SIZE, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.fc1 =nn.Linear(input_dim, 4096)\n",
        "        self.bn1 =nn.BatchNorm1d(4096)\n",
        "        self.fc2 =nn.Linear(4096, 4096)\n",
        "        self.bn2 = nn.BatchNorm1d(4096)\n",
        "        self.fc3 =nn.Linear(4096, 2048)\n",
        "        self.bn3= nn.BatchNorm1d(2048)\n",
        "        self.fc4 = nn.Linear(2048, 1024)\n",
        "        self.bn4 =nn.BatchNorm1d(1024)\n",
        "        self.fc5= nn.Linear(1024, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.fc6 =nn.Linear(512, 256)\n",
        "        self.bn6= nn.BatchNorm1d(256)\n",
        "        self.fc7 =nn.Linear(256, 128)\n",
        "        self.bn7= nn.BatchNorm1d(128)\n",
        "        self.fc8= nn.Linear(128, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =x.view(x.size(0), -1)\n",
        "        x =F.relu(self.bn1(self.fc1(x)))\n",
        "        x =F.relu(self.bn2(self.fc2(x)))\n",
        "        x =F.relu(self.bn3(self.fc3(x)))\n",
        "        x= F.relu(self.bn4(self.fc4(x)))\n",
        "        x= F.relu(self.bn5(self.fc5(x)))\n",
        "        x= F.relu(self.bn6(self.fc6(x)))\n",
        "        x=F.relu(self.bn7(self.fc7(x)))\n",
        "        x = self.fc8(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06a6ab69"
      },
      "outputs": [],
      "source": [
        "def train_ablation_experiment(model_cls, experiment_name, optimizer_type=\"adam\", lr=LR, epochs=EPOCHS):\n",
        "    model = model_cls().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if optimizer_type.lower()==\"adam\":\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    elif optimizer_type.lower()==\"sgd\":\n",
        "        optimizer =torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    best_val_acc =0.0\n",
        "    final_val_acc =0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss=0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images =images.to(device, non_blocking=True)\n",
        "            labels=labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            outputs =model(images)\n",
        "            loss= criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss +=loss.item()*images.size(0)\n",
        "\n",
        "        epoch_loss = running_loss /len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images =images.to(device, non_blocking=True)\n",
        "                labels =labels.to(device, non_blocking=True)\n",
        "                outputs= model(images)\n",
        "                loss=criterion(outputs, labels)\n",
        "\n",
        "                val_running_loss+= loss.item()*images.size(0)\n",
        "                _, preds = outputs.max(1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_loss=val_running_loss /len(val_loader.dataset)\n",
        "        val_acc=correct / total if total > 0 else 0.0\n",
        "        final_val_acc = val_acc\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch==0:\n",
        "            print(f\"[{experiment_name}] Epoch {epoch+1}/{epochs}-\"\n",
        "                  f\"Train Loss:{epoch_loss:.4f},Val Loss: {val_loss:.4f},\"\n",
        "                  f\"Val Acc:{val_acc*100:.2f}%\")\n",
        "\n",
        "    return final_val_acc, best_val_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E8v97X5gWQ9",
        "outputId": "6cec7ef9-26aa-426f-dbea-c908d031013c"
      },
      "outputs": [],
      "source": [
        "print(\"Running Ablation Study Experiments \\n\")\n",
        "\n",
        "\n",
        "# Baseline: ReLU+BN+Dropout, Adam, LR=1e-3\n",
        "print(\"\\n1. Baseline: ReLU+BN+Dropout, Adam, LR=1e-3\")\n",
        "baseline_final, baseline_best =train_ablation_experiment(\n",
        "    DeepFCN_ReLU_BN_Dropout, \"Baseline (Dropout)\", optimizer_type=\"adam\", lr=1e-3\n",
        ")\n",
        "\n",
        "# Ablation 1: Remove Dropout\n",
        "print(\"\\n2. Ablation: Remove Dropout (ReLU+BN only, Adam, LR=1e-3)\")\n",
        "no_dropout_final, no_dropout_best = train_ablation_experiment(\n",
        "    DeepFCN_ReLU_BN_NoDropout, \"No Dropout\", optimizer_type=\"adam\", lr=1e-3\n",
        ")\n",
        "\n",
        "# Ablation 2: LR too high (10x)\n",
        "print(\"\\n3. Ablation: LR too high (ReLU+BN+Dropout, Adam, LR=1e-2)\")\n",
        "lr_high_final, lr_high_best = train_ablation_experiment(\n",
        "    DeepFCN_ReLU_BN_Dropout, \"LR=1e-2 (High)\", optimizer_type=\"adam\", lr=1e-2\n",
        ")\n",
        "\n",
        "# Ablation 3: LR too low (0.1x)\n",
        "print(\"\\n4. Ablation: LR too low (ReLU+BN+Dropout, Adam, LR=1e-4)\")\n",
        "lr_low_final, lr_low_best = train_ablation_experiment(\n",
        "    DeepFCN_ReLU_BN_Dropout, \"LR=1e-4 (Low)\", optimizer_type=\"adam\", lr=1e-4\n",
        ")\n",
        "\n",
        "# Ablation 4: Vanilla SGD instead of Adam\n",
        "print(\"\\n5. Ablation: Vanilla SGD (ReLU+BN+Dropout, SGD, LR=1e-3)\")\n",
        "sgd_final, sgd_best = train_ablation_experiment(\n",
        "    DeepFCN_ReLU_BN_Dropout, \"SGD\", optimizer_type=\"sgd\", lr=1e-3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45e96e1f",
        "outputId": "dac5ac91-b1c1-4b63-e7ca-11f449be4e6d"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"Experiment\": [\n",
        "        \"Baseline (ReLU+BN+Dropout, Adam, LR=1e-3)\",\n",
        "        \"Remove Dropout\",\n",
        "        \"LR too high (1e-2)\",\n",
        "        \"LR too low (1e-4)\",\n",
        "        \"Vanilla SGD (instead of Adam)\"\n",
        "    ],\n",
        "    \"Final Val Accuracy (%)\": [\n",
        "        baseline_final * 100,\n",
        "        no_dropout_final*100,\n",
        "        lr_high_final * 100,\n",
        "        lr_low_final * 100,\n",
        "        sgd_final * 100\n",
        "    ],\n",
        "    \"Best Val Accuracy (%)\": [\n",
        "        baseline_best*100,\n",
        "        no_dropout_best * 100,\n",
        "        lr_high_best * 100,\n",
        "        lr_low_best * 100,\n",
        "        sgd_best * 100\n",
        "    ],\n",
        "    \"Change from Baseline (%)\": [\n",
        "        0.0,\n",
        "        (no_dropout_final-baseline_final)*100,\n",
        "        (lr_high_final-baseline_final) * 100,\n",
        "        (lr_low_final - baseline_final) *100,\n",
        "        (sgd_final -baseline_final) *100\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results[\"Final Val Accuracy (%)\"] = df_results[\"Final Val Accuracy (%)\"].round(2)\n",
        "df_results[\"Best Val Accuracy (%)\"] = df_results[\"Best Val Accuracy (%)\"].round(2)\n",
        "df_results[\"Change from Baseline (%)\"] = df_results[\"Change from Baseline (%)\"].round(2)\n",
        "\n",
        "print(\"ABLATION STUDY SUMMARY TABLE\")\n",
        "print(df_results.to_string(index=False))\n",
        "\n",
        "changes = {\n",
        "    \"Remove Dropout\": abs(df_results.loc[1, \"Change from Baseline (%)\"]),\n",
        "    \"LR too high\": abs(df_results.loc[2, \"Change from Baseline (%)\"]),\n",
        "    \"LR too low\": abs(df_results.loc[3, \"Change from Baseline (%)\"]),\n",
        "    \"Vanilla SGD\": abs(df_results.loc[4, \"Change from Baseline (%)\"])\n",
        "}\n",
        "\n",
        "biggest_impact = max(changes, key=changes.get)\n",
        "print(f\"\\nBiggest Impact: {biggest_impact} (change: {changes[biggest_impact]:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "4719a303",
        "outputId": "0d132299-1611-4476-c200-e478da399e85"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "experiments = df_results[\"Experiment\"].tolist()\n",
        "final_accs = df_results[\"Final Val Accuracy (%)\"].tolist()\n",
        "changes = df_results[\"Change from Baseline (%)\"].tolist()\n",
        "\n",
        "# Plot 1:Final Validation Accuracy\n",
        "ax1.barh(experiments, final_accs, color=['green', 'blue', 'orange', 'red', 'purple'])\n",
        "ax1.set_xlabel(\"Final Validation Accuracy (%)\")\n",
        "ax1.set_title(\"Final Validation Accuracy by Experiment\")\n",
        "ax1.axvline(x=baseline_final * 100, color='green', linestyle='--',alpha=0.5, label='Baseline')\n",
        "ax1.legend()\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Plot 2:Change from Baseline\n",
        "colors_change = ['gray' if abs(c) < 1 else ('red' if c<0 else 'blue') for c in changes]\n",
        "ax2.barh(experiments, changes, color=colors_change)\n",
        "ax2.set_xlabel(\"Change from Baseline (%)\")\n",
        "ax2.set_title(\"Impact on Performance (Change from Baseline)\")\n",
        "ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "ax2.grid(axis='x',alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
