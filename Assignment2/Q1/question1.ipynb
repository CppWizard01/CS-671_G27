{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "54bb644e",
      "metadata": {
        "id": "54bb644e"
      },
      "source": [
        "# Question 1.1: The Manual Dimension Map\n",
        "\n",
        "## Network Configuration\n",
        "* **Input:** 64 x 64 x 3 (Tiny ImageNet)\n",
        "* **Layers:** 3 Convolutional, 2 Max-Pooling, 1 Fully Connected\n",
        "* **Formula:** $H_{out} = \\lfloor \\frac{H_{in} + 2P - K}{S} \\rfloor + 1$\n",
        "\n",
        "---\n",
        "\n",
        "## Layer-by-Layer Breakdown\n",
        "\n",
        "### 1. Input Layer\n",
        "* **Dimensions:** 64 x 64 x 3\n",
        "\n",
        "### 2. Conv1 Layer\n",
        "* **Parameters:** 3x3 kernel, Padding=1, Stride=1, 32 Filters\n",
        "* **Calculation:** $(64 + 2*1 - 3)/1 + 1 = 64$\n",
        "* **Output Shape:** **64 x 64 x 32**\n",
        "\n",
        "### 3. Pool1 Layer\n",
        "* **Parameters:** 2x2 kernel, Stride=2\n",
        "* **Calculation:** $(64 - 2)/2 + 1 = 32$\n",
        "* **Output Shape:** **32 x 32 x 32**\n",
        "\n",
        "### 4. Conv2 Layer\n",
        "* **Parameters:** 3x3 kernel, Padding=1, Stride=1, 64 Filters\n",
        "* **Calculation:** $(32 + 2*1 - 3)/1 + 1 = 32$\n",
        "* **Output Shape:** **32 x 32 x 64**\n",
        "\n",
        "### 5. Pool2 Layer\n",
        "* **Parameters:** 2x2 kernel, Stride=2\n",
        "* **Calculation:** $(32 - 2)/2 + 1 = 16$\n",
        "* **Output Shape:** **16 x 16 x 64**\n",
        "\n",
        "### 6. Conv3 Layer\n",
        "* **Parameters:** 3x3 kernel, Padding=1, Stride=1, 128 Filters\n",
        "* **Calculation:** $(16 + 2*1 - 3)/1 + 1 = 16$\n",
        "* **Output Shape:** **16 x 16 x 128**\n",
        "\n",
        "### 7. Flattening & FC Layer\n",
        "* **Flatten:** $16 \\times 16 \\times 128 = 32,768$ features\n",
        "* **FC Layer:** 32,768 units $\\to$ 10 (Classes)\n",
        "* **Final Output:** **10**\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Layer | Input | Operation | Output |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| Input | - | - | 64 x 64 x 3 |\n",
        "| Conv1 | 64 x 64 x 3 | 3x3, p=1 | 64 x 64 x 32 |\n",
        "| Pool1 | 64 x 64 x 32 | 2x2, s=2 | 32 x 32 x 32 |\n",
        "| Conv2 | 32 x 32 x 32 | 3x3, p=1 | 32 x 32 x 64 |\n",
        "| Pool2 | 32 x 32 x 64 | 2x2, s=2 | 16 x 16 x 64 |\n",
        "| Conv3 | 16 x 16 x 64 | 3x3, p=1 | 16 x 16 x 128 |\n",
        "| FC | 32,768 | Linear | 10 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9f4543aa",
      "metadata": {
        "id": "9f4543aa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1ac831",
      "metadata": {
        "id": "7d1ac831"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "\n",
        "    # We have implemeneted a Custom CNN with explicit forward pass (not Sequential) here\n",
        "    # The Architecture: Conv1 -> Pool1 -> Conv2 -> Pool2 -> Conv3 -> FC\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        # Padding =1 to preserve spatial size\n",
        "        self.conv1=nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2= nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv3=nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        # MaxPool 2x2 stride 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Fully connected which matches flattened size after two poolings (16x16x128)\n",
        "        self.fc = nn.Linear(16 * 16 * 128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"Input:  {x.shape}\")           # (N, 3, 64, 64)\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        print(f\"After Conv1:{x.shape}\")           # (N, 32, 64, 64)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        print(f\"After Pool1:{x.shape}\") # (N, 32, 32, 32)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        print(f\"After Conv2:{x.shape}\")           # (N, 64, 32, 32)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        print(f\"After Pool2:{x.shape}\")    # (N, 64, 16, 16)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        print(f\"After Conv3:{x.shape}\")           # (N, 128, 16, 16)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        print(f\"Flatten: {x.shape}\")           # (N, 32768)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        print(f\"FC Output: {x.shape}\")      # (N, 10)\n",
        "        return x\n",
        "\n",
        "#This function counts the trainable parameters\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac0b16a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ac0b16a",
        "outputId": "05b66a28-00c1-4cac-b07b-b626cfaae4c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters: 420,938\n",
            "Layer-by-layer shape verification\n",
            "Input:  torch.Size([1, 3, 64, 64])\n",
            "After Conv1:torch.Size([1, 32, 64, 64])\n",
            "After Pool1:torch.Size([1, 32, 32, 32])\n",
            "After Conv2:torch.Size([1, 64, 32, 32])\n",
            "After Pool2:torch.Size([1, 64, 16, 16])\n",
            "After Conv3:torch.Size([1, 128, 16, 16])\n",
            "Flatten: torch.Size([1, 32768])\n",
            "FC Output: torch.Size([1, 10])\n",
            "\n",
            "Final output shape: torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "model=SimpleCNN(num_classes=10)\n",
        "print(f\"Total trainable parameters: {count_parameters(model):,}\")\n",
        "print(\"Layer-by-layer shape verification\")\n",
        "\n",
        "dummy_input=torch.randn(1, 3, 64, 64)\n",
        "output=model(dummy_input)\n",
        "\n",
        "print(f\"\\nFinal output shape:{output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j5atXUzOWD_H",
      "metadata": {
        "id": "j5atXUzOWD_H"
      },
      "source": [
        "# Parameter Explosion Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ce65acac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce65acac",
        "outputId": "d8bc97f4-538e-48bc-8485-6996ee9653c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WITH POOLING:\n",
            "Feature map before FC: 16 x 16 x 128 = 32,768 features\n",
            "FC layer parameters: 327,690\n",
            "WITHOUT POOLING:\n",
            "Feature map before FC: 64 x 64 x 128 = 524,288 features\n",
            "FC layer parameters: 5,242,890\n",
            "PARAMETER EXPLOSION:\n",
            "Increase factor: 16.0x\n",
            "Additional parameters: 4,915,200\n"
          ]
        }
      ],
      "source": [
        "# WITH POOLING using the current architecture\n",
        "fc_input_with_pool = 16 * 16 * 128  # After 2 pooling layers\n",
        "fc_params_with_pool = fc_input_with_pool * 10 + 10  # weights + bias\n",
        "\n",
        "# WITHOUT POOLING ==> no spatial reduction\n",
        "fc_input_no_pool = 64 * 64 * 128  # Original spatial size preserved\n",
        "fc_params_no_pool = fc_input_no_pool * 10 + 10  # weights + bias\n",
        "\n",
        "print(f\"WITH POOLING:\")\n",
        "print(f\"Feature map before FC: 16 x 16 x 128 = {fc_input_with_pool:,} features\")\n",
        "print(f\"FC layer parameters: {fc_params_with_pool:,}\")\n",
        "\n",
        "print(f\"WITHOUT POOLING:\")\n",
        "print(f\"Feature map before FC: 64 x 64 x 128 = {fc_input_no_pool:,} features\")\n",
        "print(f\"FC layer parameters: {fc_params_no_pool:,}\")\n",
        "\n",
        "print(f\"PARAMETER EXPLOSION:\")\n",
        "print(f\"Increase factor: {fc_params_no_pool / fc_params_with_pool:.1f}x\")\n",
        "print(f\"Additional parameters: {fc_params_no_pool - fc_params_with_pool:,}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XSVglvjHWpSn",
      "metadata": {
        "id": "XSVglvjHWpSn"
      },
      "source": [
        "# Why is this a problem?\n",
        "\n",
        "High parameter counts, especially in the transition to Fully Connected (FC) layers, create several bottlenecks that can ruin a model's performance and efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Memory Constraints\n",
        "Every parameter in a model requires dedicated space in the GPU or RAM. For instance, a single FC layer with 5.2 million parameters (using 32-bit floats) consumes about 20MB of memory just for the weights alone.\n",
        "\n",
        "### 2. Computational Latency\n",
        "More parameters mean more multiply-accumulate operations during every forward and backward pass. This makes training significantly slower and increases inference time, which is a major issue for real-time applications.\n",
        "\n",
        "### 3. The Overfitting Trap\n",
        "When a model has too much \"capacity\" (too many parameters) relative to the complexity of the task, it stops learning patterns and starts memorizing noise.\n",
        "* **Memorization:** The model learns the specific pixels of the training set rather than the general features.\n",
        "* **Generalization:** It will perform perfectly on your training data but fail on any unseen data.\n",
        "* **Regularization:** You end up needing aggressive techniques like Dropout or Weight Decay just to keep the model from spiraling.\n",
        "\n",
        "### 4. Data Requirements\n",
        "A common rule of thumb is that you should have roughly 10x more training samples than you have parameters to ensure the model generalizes well. If you have millions of parameters but only thousands of images, the model is mathematically over-determined.\n",
        "\n",
        "---\n",
        "\n",
        "## The Solution: Strategic Pooling\n",
        "\n",
        "Pooling layers (Max or Average) are used to solve these issues by progressively reducing the spatial dimensions (Width x Height) of the feature maps.\n",
        "\n",
        "By downsampling, we:\n",
        "1.  **Reduce the Flattened Vector:** This keeps the input to the FC layer manageable.\n",
        "2.  **Focus on Features:** We preserve the most important information while discarding redundant spatial data.\n",
        "3.  **Efficiency:** This allows the network to remain deep and accurate without blowing the memory or data budget."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "wd8RkWvBWqXm",
      "metadata": {
        "id": "wd8RkWvBWqXm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4f6a476d",
      "metadata": {
        "id": "4f6a476d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
